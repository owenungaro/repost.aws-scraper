{
  "body": "Hello,\nI am trying to create a Node Group in an Amazon EKS cluster and I consistently receive the following main error:\nNodeCreationFailure: Unhealthy nodes in the kubernetes cluster\nAdditionally, I am seeing the following technical warning:\nInvalidDiskCapacity invalid capacity 0 on image filesystem\nI have already tried all recommended solutions, including:\nCreating the EKS cluster with version 1.33 (also tested in us-east-1)\nNode Group using Amazon Linux 2023 (x86_64) Standard (AL2023_x86_64_STANDARD)\nDisk size set to 20 GiB\nNode Group launched in a public subnet with auto-assign public IPv4 address enabled\nSecurity Groups and IAM Roles correctly configured (AmazonEKSWorkerNodePolicy, AmazonEC2ContainerRegistryReadOnly, AmazonEKS_CNI_Policy)\nTested with only one public subnet\nMy account has sufficient EC2 and VPC limits/quotas\nRecreated VPCs and subnets from scratch (always public)\nTried creating the Node Group both from the AWS Console and with eksctl\nThe EC2 nodes are launched but remain \"unhealthy\" in the EKS cluster.\nPlease assist me in reviewing the backend logs and possible account-related restrictions or issues, as all evidence suggests this is not a user configuration error.\nI can provide screenshots or additional details if needed.\nThank you very much.\n\nIs this a warning or an error. If a warning, you can continue. The root cause is usually related to the node not having access to its EBS volume size information on initial startup.\n\nBased on the issues you're experiencing with your EKS node group creation, there are several potential causes for the \"NodeCreationFailure: Unhealthy nodes in the kubernetes cluster\" error and the \"InvalidDiskCapacity invalid capacity 0 on image filesystem\" warning.\nThe most likely issue relates to the bootstrap script in the UserData configuration of your worker nodes. This script is critical for nodes to properly join the EKS cluster, and it must contain the correct EKS cluster name. When this configuration is incorrect, nodes can launch as EC2 instances but fail to register with the Kubernetes cluster.\nHere are some specific troubleshooting steps to resolve this issue:\nVerify the UserData configuration for your worker nodes. Ensure the bootstrap script is present and correctly formatted with your exact cluster name:\n#!/bin/bash\n/etc/eks/bootstrap.sh your-cluster-name\nCheck if there are any capacity issues with the disk. The \"InvalidDiskCapacity invalid capacity 0\" warning suggests there might be a problem with the disk allocation or filesystem configuration on the nodes.\nReview the CloudFormation stack events for the node group creation to identify specific error messages.\nEnsure your VPC and subnet configurations have the correct DHCP options set, including domain-name and domain-name-servers.\nVerify that the AWS STS endpoint for your region is enabled for your account.\nCheck that security groups allow necessary traffic between the nodes and the control plane, particularly on port 50051 which is used for node-to-control-plane communication.\nIf you're using a managed node group, verify the node group configuration in the EKS console to ensure the correct cluster name is specified.\nTry creating a node group in a different region to determine if the issue is region-specific.\nCheck the CNI plugin version compatibility with your Kubernetes version. An incompatible or misconfigured CNI plugin can prevent nodes from becoming ready.\nVerify that kube-proxy is running correctly by checking its pod status in the kube-system namespace.\nIf these steps don't resolve the issue, you may need to collect more detailed logs from the worker nodes to identify the specific cause of the failure.\nSources\nEKS node creation failed | AWS re:Post\nEKS Nodes Fail to Create - but why? It worked yesterday? | AWS re:Post\nEKS problem after upgrade from 1.23 | AWS re:Post"
}