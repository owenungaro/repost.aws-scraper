{
  "body": "This article provides a brief guide on implementing Kubernetes Network Policies in Amazon EKS using the AWS VPC CNI plugin, aiming to enhance pod-level traffic control and security within your cluster.\n\nOverview of steps to configure Network Policies in EKS\nUpdate to the latest Amazon VPC CNI version\nEnable network policy support in CNI Configuration\na. If you are using eksctl, you can achieve this with below in the cluster config file\naddons:\n       - name: vpc-cni\n         version: 1.14.0\n         configurationValues: |-\n           enableNetworkPolicy: \"true\"\nSet up the necessary IAM permissions\nDeploy the network policy controller\na. When network policies are enabled, the EKS control plane automatically installs the network policy controller. This controller monitors the creation of network\npolicies and configures the necessary eBPF programs on the worker nodes.\nDefine and apply network policies\na. Below Network Policy denies all ingress traffic by default\napiVersion: networking.k8s.io/v1\n     kind: NetworkPolicy\n     metadata:\n       name: default-deny-all\n     spec:\n       podSelector: {}\n       policyTypes:\n       - Ingress\nDeploy application\nAmazon VPC CNI Network Policies workflow:\n1. NetworkPolicy object creation (EKS Cluster administrator creates network policy)\nThe process begins when an EKS Cluster Administrator creates a Kubernetes NetworkPolicy object using kubectl or another Kubernetes management tool. The administrator authenticates using their Kubernetes username and must have appropriate permissions. The administrator's NetworkPolicy manifest is applied to the Kubernetes API server, which creates or updates the NetworkPolicy resource in the Kubernetes data store.\nRBAC verification for Network Policy Controller (Authentication and authorization) -The Kubernetes API server verifies that the Network Policy Controller has proper authorization via the \"eks:network-policy-controller\" ClusterRole to watch and process NetworkPolicy objects.\n2. NetworkPolicy event streaming via Watch API\nThe Network Policy Controller (running in EKS Control Plane) receives the NetworkPolicy object event through its established Watch API connection with the Kubernetes API server. This streaming connection allows real-time updates without polling.\nThe Network Policy Controller parses the NetworkPolicy rules, which specify which pods can communicate with each other based on labels, namespaces, ports, and protocols.\n3. Policy distribution to worker nodes\nThe Network Policy Controller distributes the processed policy information to the Network Policy Agent DaemonSet running on each worker node in the cluster.\n4. Policy translation by Network Policy Agent\nThe Network Policy Agent translates the Kubernetes NetworkPolicy rules into a format that can be implemented using both AWS Security Groups and eBPF programs.\n6. Security Group creation and updates\nThe VPC CNI Plugin creates or updates AWS Security Groups based on the translated policy rules. These security groups will be associated with pod ENIs (Elastic Network Interfaces) to enforce network policies at the AWS infrastructure level.\n7. eBPF program loading\nThe VPC CNI Plugin compiles and loads eBPF programs into the Linux kernel on the worker node. These programs will intercept and filter network traffic at the kernel level.\nThe eBPF programs update BPF maps (kernel data structures) with policy information. These maps store relationships between pods and their applicable policies, allowed/denied traffic patterns, and other policy metadata.\n8 -9 . Pod network traffic interception\nWhen a pod sends or receives network packets, the eBPF programs intercept this traffic in the kernel before it leaves the host or reaches its destination.\nThe eBPF programs query the BPF maps to determine which rules apply to the specific network packet based on source/destination pods, protocols, ports, etc.\n10. Traffic filtering decision\nBased on the applicable rules:\nIf the traffic is allowed by the network policy, the eBPF program permits the packet to proceed to its destination.\nIf the traffic is denied by the network policy, the eBPF program drops the packet, preventing communication.\nNetwork Policies troubleshooting steps:\nReview the NetworkPolicies created by customer on EKS Cluster\nDescribe VPC CNI daemonset or Addon and validate the Network Policy mode enabled by the customer - strict vs enforced mode.\nApply the same policies on your lab cluster and Test if the network policy is working  in lab cluster\na. If the policies are not working only on customer cluster, check if customer installed any Infra add-ons and security tools like Kyverno, Open Policy Agent that could have Cluster Policies configured and not allowing Network Policy Controller (running on EKS ControlPlane) to access the policies\nExecute EKS Log collector script and check network-policy-agent.log\nNetwork Policy Agent exposes prometheus metrics at port 61680\na.\nhttps://github.com/aws/aws-network-policy-agent/blob/d85dbabad07b2919422c0f95172854da75e3c087/pkg/metrics/metrics.go#L18C16-L18C21\nEKS VPC CNI Network policy agent known issues and root causes :\nError: EKS vpc-cni is generating the networkpolicy logs, given the enable-policy-event-logs is set to false.\nRoot Cause:\nThe enable-policy-event-logs setting won't disable the Network Policy (NP) agent logging, but it will only disable the policy \"decision\" logs. This is documented in the aws-network-policy-agent README.\nError: Network policy map cleanup\nRoot Cause:\nThe issue was caused by a problem with the VPC CNI addon version 1.19.3-eksbuild.1.\nError: The customer has enabled the network policy feature in the AWS VPC (Virtual Private Cloud) CNI (Container Network Interface) plugin, but they are still seeing issues with the network policies not being applied correctly.\nRoot Cause:\nThis appears to be a customer-specific issue with Kyverno. The root cause is that the policyendpoint object was not created in the namespace, which meant the NP controller was unable to create network policy rules for the NP agent to apply.\nError: When you have network policies enabled in strict mode, pods will come up with default deny policy. Once we have policies applied, traffic will be allowed to the allowed endpoints. When policies are deleted the pod does not go back to default deny state, it goes to default allow state.\nRoot Cause:\nThis issue was fixed in the 1.19.3 VPC CNI release, which included the NP agent 1.2.0 release. After the fix, with strict mode, once the policies were removed, the pod would fall back to the default deny state\nError: The customer is using the Security Groups for Pods (SGFP) feature in EKS, which is known to have some limitations around pod startup latency due to rate limiting in the resource controller.\nRoot Cause:\nThe root cause of the issue was identified as API throttling on the CreateNetworkInterface API, which the VPC resource controller uses to create branch ENIs for the pods. The customer's account had a low limit for this API, which was causing delays during high-scale events.\nErrors: FailedScheduling 2m53s (x28 over 137m) default-scheduler 0/5 nodes are available: 5 Insufficient vpc.amazonaws.com/pod-eni. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.\nRoot Cause:\nAssigning Security Groups to pods will increase pod scheduling latency, and HyperPod should consider running its own managed controller like the Node and Certificate controllers.\nErrors: {\"level\":\"info\",\"ts\":\"2022-02-04T22:24:55.014Z\",\"caller\":\"entrypoint.sh\",\"msg\":\"Checking for IPAM connectivity ... \"} I0204 22:24:56.095582      12 request.go:621] Throttling request took 1.047064274s, request: GET:\nhttps://10.100.0.1:443/apis/coordination.k8s.io/v1beta1?timeout=32s\n{\"level\":\"info\",\"ts\":\"2022-02-04T22:24:57.022Z\",\"caller\":\"entrypoint.sh\",\"msg\":\"Retrying waiting for IPAM-D\"}panic: runtime error: invalid memory address onil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x39 pc=0x5597a7c2b3f8]\nRoot Cause:\nThis issue occurs when a customer installs systemd-udev on AL2023, as the file is re-written with a breaking policy. This can happen when updating to a different releasever that has an updated package or manually updating the package itself.\nError: {\"level\":\"error\",\"ts\":\"2025-02-05T20:27:18.669Z\",\"caller\":\"ebpf/bpf_client.go:578\",\"msg\":\"failed to find device by name eni9ea69618bf0: %!w(netlink.LinkNotFoundError={0xc000115310})\"}\nRoot Cause:\nThis issue has been identified and fixed in the latest versions of the AWS VPC CNI network policy agent (v1.2.0).\nError: Enhanced EKSImageScan CVE Report for CVE\nhttps://nvd.nist.gov/vuln/detail/CVE-2025-22868\nRoot Cause:\nThe ticket submitter suggests that the scanner can be updated to address the vulnerabilities found in the previous Multus CNI image, version v4.1.4-eksbuild.2_thick. However, since the new version of the Multus CNI image has no vulnerabilities, the next step would be to update the systems to use the new version of the Multus CNI image and the new Network Policy Controller image.\nError: {\"level\":\"info\",\"ts\":\"2024-11-25T13:34:24.808Z\",\"logger\":\"ebpf-client\",\"caller\":\"events/events.go:193\",\"msg\":\"Flow Info: \",\"Src IP\":\"\",\"Src Port\":9096,\"Dest IP\":\"\",\"Dest Port\":56830,\"Proto\":\"TCP\",\"Verdict\":\"DENY\"}\nRoot Cause:\nThe issue has been resolved in the new version of the Network Policy Controller (NPC).\nError: Customer has an EKS cluster that they recently upgraded to 1.30 and switched from Calico to AWS VPC CNI. After the switch from Calico to AWS VPC CNI they started having networking issues for pod to pod communication. Previous engineer went through troubleshooting with the customer and noticed when they delete the networking policies that the communication is restablished. When the networking policies are reapplied the communication stops working again.\nRoot Cause:\nThe issue was resolved by using port ranges in the network policies, as the maximum number of unique combinations of ports for each protocol in each ingress: or egress: selector in a network policy is 24. The customer was advised to use port ranges to reduce the number of unique ports and avoid the limitation.\nError: Network policy agent does not support Standalone Pods\nRoot Cause:\nThe Network Policy agent currently only supports pods that are deployed as part of a deployment/replicaset. If network policies are applied to standalone pods, there might be some inconsistencies in the behavior, as documented in the aws-network-policy-agent GitHub issue #327\nAuthors : Sai Teja P, Dashmeet"
}