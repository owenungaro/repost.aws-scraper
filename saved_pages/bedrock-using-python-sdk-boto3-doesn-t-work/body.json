{
  "body": "I run this code\nimport os\nimport boto3\nimport json\nfrom botocore.exceptions import ClientError\ndef ask_bedrock_question(question, model_id):\n\"\"\"\nAsk a question to AWS Bedrock and get a response\n\"\"\"\nbedrock_runtime = boto3.client(\nservice_name='bedrock-runtime',\nregion_name='eu-west-3'\n)\n# Use the correct request format for different model families\nif \"anthropic\" in model_id:\n    body = {\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"max_tokens\": 1000,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": question\n            }\n        ]\n    }\nelif \"amazon.nova\" in model_id:\n    body = {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"text\": question}]\n            }\n        ],\n        \"inferenceConfig\": {\n            \"max_new_tokens\": 1000\n        }\n    }\nelif \"amazon.titan-text\" in model_id:\n    body = {\n        \"inputText\": question,\n        \"textGenerationConfig\": {\n            \"maxTokenCount\": 1000,\n            \"temperature\": 0.7\n        }\n    }\nelse:\n    # Generic format for other models\n    body = {\n        \"prompt\": question,\n        \"max_tokens\": 1000,\n        \"temperature\": 0.7\n    }\n\ntry:\n    response = bedrock_runtime.invoke_model(\n        body=json.dumps(body),\n        modelId=model_id,\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n\n    response_body = json.loads(response.get('body').read())\n\n    # Parse response based on model type\n    if \"anthropic\" in model_id:\n        answer = response_body['content'][0]['text']\n    elif \"amazon.nova\" in model_id:\n        answer = response_body['output']['message']['content'][0]['text']\n    elif \"amazon.titan-text\" in model_id:\n        answer = response_body['results'][0]['outputText']\n    else:\n        # Try to find text in common response fields\n        answer = (response_body.get('completion') or\n                  response_body.get('text') or\n                  response_body.get('generated_text') or\n                  str(response_body))\n\n    return answer\n\nexcept Exception as e:\n    return f\"Error: {str(e)}\"\ndef list_inference_profiles():\n\"\"\"List available inference profiles\"\"\"\ntry:\nbedrock = boto3.client('bedrock', region_name='us-east-1')\nresponse = bedrock.list_inference_profiles()\nprint(\"=== Available Inference Profiles ===\")\n    profiles = []\n    for profile in response.get('inferenceProfileSummaries', []):\n        profile_info = {\n            'id': profile['inferenceProfileId'],\n            'name': profile.get('inferenceProfileName', 'N/A'),\n            'models': profile.get('models', [])\n        }\n        profiles.append(profile_info)\n        print(f\"Profile: {profile_info['id']}\")\n        print(f\"  Name: {profile_info['name']}\")\n        if profile_info['models']:\n            print(f\"  Models: {[m.get('modelId', 'N/A') for m in profile_info['models']]}\")\n        print()\n\n    return profiles\n\nexcept Exception as e:\n    print(f\"Error listing inference profiles: {e}\")\n    return []\ndef test_models_and_profiles():\n\"\"\"Test both direct model IDs and inference profiles\"\"\"\n# First try older, stable Claude models that should work with direct IDs\nstable_models = [\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    \"anthropic.claude-3-5-haiku-20241022-v1:0\",\n]\n\n# Test Amazon models (usually more reliable)\namazon_models = [\n    \"amazon.nova-lite-v1:0\",\n    \"amazon.nova-micro-v1:0\",\n    \"amazon.titan-text-express-v1\",\n    \"amazon.titan-text-lite-v1\",\n]\n\n# Test other models\nother_models = [\n    \"cohere.command-r-v1:0\",\n    \"meta.llama3-2-3b-instruct-v1:0\",\n    \"meta.llama3-1-8b-instruct-v1:0\",\n]\n\nall_models = stable_models + amazon_models + other_models\nquestion = \"Hello, how are you?\"\n\nprint(\"=== Testing Direct Model IDs ===\")\nfor model_id in all_models:\n    print(f\"\\n\ud83e\uddea Testing: {model_id}\")\n    result = ask_bedrock_question(question, model_id)\n\n    if not result.startswith(\"Error:\"):\n        print(f\"\u2705 SUCCESS with {model_id}\")\n        print(f\"Response: {result[:150]}...\")\n        return model_id, \"direct\"\n    else:\n        print(f\"\u274c Failed: {result[:100]}...\")\n\nprint(\"\\n=== Testing Inference Profiles ===\")\nprofiles = list_inference_profiles()\n\nfor profile in profiles:\n    profile_id = profile['id']\n    print(f\"\\n\ud83e\uddea Testing profile: {profile_id}\")\n    result = ask_bedrock_question(question, profile_id)\n\n    if not result.startswith(\"Error:\"):\n        print(f\"\u2705 SUCCESS with profile {profile_id}\")\n        print(f\"Response: {result[:150]}...\")\n        return profile_id, \"profile\"\n    else:\n        print(f\"\u274c Failed: {result[:100]}...\")\n\nreturn None, None\nif\nname\n== \"\nmain\n\":\nworking_model, model_type = test_models_and_profiles()\nif working_model:\n    print(f\"\\n\ud83c\udf89 SUCCESS! Use this {model_type}: {working_model}\")\n\n    # Test with your original question\n    question = \"What are the benefits of cloud computing?\"\n    answer = ask_bedrock_question(question, working_model)\n    print(f\"\\nFinal test:\")\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n\n    # Provide the working code\n    print(f\"\\n\ud83d\udcdd Your working code:\")\n    print(f'model_id = \"{working_model}\"')\n    print(f'answer = ask_bedrock_question(\"Your question here\", model_id)')\n\nelse:\n    print(\"\\n\u274c No models or profiles worked.\")\n    print(\"\\n\ud83d\udd27 Try these steps:\")\n    print(\"1. Wait 30 minutes and try again\")\n    print(\"2. Try a different region: us-west-2 or eu-west-1\")\n    print(\"3. Check your AWS account Bedrock quotas\")\n    print(\"4. Contact AWS Support\")\nAnd getting this output:\n<summarized>\n\u274c No models or profiles worked.\n</summarized>\nRegion is fine, AWS creds are fine. In fact, the user is an admin user.\nWhat am I doing wrong, what am I overlooking?\n\nBased on your code and the error you're experiencing, there are a few issues that could be causing the problem:\nAPI Usage\n: You're using\ninvoke_model\nwith different request body formats, but the code might not be correctly formatted for each model. For Amazon Bedrock, it's recommended to use the newer\nconverse\nAPI for most models, especially when working with conversation-style interactions.\nRegion Availability\n: You're using\neu-west-3\nfor the bedrock-runtime client but\nus-east-1\nfor listing inference profiles. Not all Bedrock models are available in all regions. Make sure the models you're trying to use are available in\neu-west-3\n.\nRequest Format\n: The request format for different models varies significantly. For example, the Anthropic Claude models require specific formatting with the\nanthropic_version\nparameter.\nError Handling\n: Your code catches exceptions but doesn't provide detailed error information that might help diagnose the issue.\nHere's a simplified approach using the recommended\nconverse\nAPI for most modern models:\nimport\nboto3\nfrom\nbotocore\n.\nexceptions\nimport\nClientError\n# Create a Bedrock Runtime client\nclient\n=\nboto3\n.\nclient\n(\n\"bedrock-runtime\"\n,\nregion_name\n=\n\"eu-west-3\"\n)\n# Set the model ID (try one of these models)\nmodel_id\n=\n\"anthropic.claude-3-haiku-20240307-v1:0\"\n# or another model ID\n# Define your question\nuser_message\n=\n\"Hello, how are you?\"\n# Create conversation format\nconversation\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n[\n{\n\"text\"\n:\nuser_message\n}\n]\n,\n}\n]\ntry\n:\n# Send the message using the converse API\nresponse\n=\nclient\n.\nconverse\n(\nmodelId\n=\nmodel_id\n,\nmessages\n=\nconversation\n,\ninferenceConfig\n=\n{\n\"maxTokens\"\n:\n500\n,\n\"temperature\"\n:\n0.5\n}\n,\n)\n# Extract and print the response text\nresponse_text\n=\nresponse\n[\n\"output\"\n]\n[\n\"message\"\n]\n[\n\"content\"\n]\n[\n0\n]\n[\n\"text\"\n]\nprint\n(\nresponse_text\n)\nexcept\nClientError\nas\ne\n:\nprint\n(\nf\"ERROR: Can't invoke '\n{\nmodel_id\n}\n'. Reason:\n{\ne\n}\n\"\n)\nIf this still doesn't work, check:\nThat your AWS account has access to the models you're trying to use\nThat you have the necessary IAM permissions for Bedrock\nThat the models are available in your chosen region\nThe specific error messages to identify permission or configuration issues\nSources\nSend and process a document with Llama on Amazon Bedrock - Amazon Bedrock\nInvoke AI21 Labs Jurassic-2 on Amazon Bedrock using Bedrock's Converse API - Amazon Bedrock\n\nMy requirements.txt:\ntorch==2.6.0\ntransformers==4.51.3\ntimm==1.0.15\nPyMuPDF==1.25.5\nPillow==11.2.1\npytesseract\nopencv-python\nboto3>=1.34.0\nbotocore>=1.34.0\npython-dotenv==1.1.0"
}