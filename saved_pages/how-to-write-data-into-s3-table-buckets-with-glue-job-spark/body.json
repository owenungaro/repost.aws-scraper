{
  "body": "I've created a s3 table bucketed named test-s3-table-bucket, and namespace's name is backend_reconciliation, and in Athena, I find the Datasource is AwsDataCatalog, and Catalog is s3tablescatalog/test-s3-table-bucket, and Database is backend_reconciliation, and then I create a table named daily_sales:\nCREATE TABLE backend_reconciliation.daily_sales ( sale_date date,  product_category string,  sales_amount double) PARTITIONED BY (month(sale_date)) TBLPROPERTIES ('table_type' = 'iceberg')\nThen I create a Glue job, want to insert a row into this table, the code is like this:\n`import sys\nimport logging\nfrom pyspark.sql import SparkSession\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import lit\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nspark = SparkSession.builder\n.config(\"spark.sql.catalog.s3tablescatalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n.config(\"spark.sql.catalog.s3tablescatalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n.config(\"spark.sql.catalog.s3tablescatalog.glue.region\", \"us-east-1\")\n.config(\"spark.sql.catalog.s3tablescatalog.warehouse\", \"s3://test-s3-table-bucket/\")\n.config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n.config(\"spark.sql.defaultCatalog\", \"s3tablescatalog\")\n.getOrCreate()\nglueContext = GlueContext(spark.sparkContext)\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\ndf = spark.createDataFrame([(\"2024-06-01\", \"electronics\", 12345.67)], [\"sale_date\", \"product_category\", \"sales_amount\"])\ndf.writeTo(\"s3tablescatalog.\ntest-s3-table-bucket.backend_reconciliation\n.daily_sales\")\n.option(\"checkpointLocation\", \"s3://abc/checkpoints/daily_sales/\")\n.tableProperty(\"format-version\", \"2\")\n.append()\nlogger.info(\"write complete.\")\njob.commit()\n`\nwhile I get the error:\nError Category: QUERY_ERROR; Failed Line Number: 34; Spark Error Class: TABLE_OR_VIEW_NOT_FOUND; AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view\ns3tablescatalog\n.\ntest-s3-table-bucket.backend_reconciliation\n.\ndaily_sales\ncannot be found. Verify the spelling and correctness of the schema and catalog.\nI had tried many approaches, as the Spark does not support \"/\", while the s3 table buckets add the prefix \"s3tablescatalog\" automatically, so all of the Catalog for s3 table buckets contains \"/\", how can I solve this?\n\nTo write data into S3 table buckets with AWS Glue Spark jobs, you need to properly configure your SparkSession and use the correct table reference format. The error you're encountering is due to incorrect table referencing in your code.\nHere's how to fix your Glue job to write to your S3 table bucket:\nFirst, update your SparkSession configuration:\nspark\n=\nSparkSession\n.\nbuilder\n.\nappName\n(\n\"WriteToS3Tables\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.extensions\"\n,\n\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.defaultCatalog\"\n,\n\"s3tables\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.catalog.s3tables\"\n,\n\"org.apache.iceberg.spark.SparkCatalog\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.catalog.s3tables.catalog-impl\"\n,\n\"org.apache.iceberg.aws.glue.GlueCatalog\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.catalog.s3tables.client.region\"\n,\n\"us-east-1\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.catalog.s3tables.glue.account-id\"\n,\n\"YOUR_ACCOUNT_ID\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.catalog.s3tables.glue.id\"\n,\n\"YOUR_ACCOUNT_ID:s3tablescatalog/test-s3-table-bucket\"\n)\n\\\n.\nconfig\n(\n\"spark.sql.catalog.s3tables.warehouse\"\n,\n\"s3://test-s3-table-bucket/\"\n)\n\\\n.\ngetOrCreate\n(\n)\nThen, use one of these approaches to write data to your table:\nOption 1: Using SQL to insert data:\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[\n(\n\"2024-06-01\"\n,\n\"electronics\"\n,\n12345.67\n)\n]\n,\n[\n\"sale_date\"\n,\n\"product_category\"\n,\n\"sales_amount\"\n]\n)\ndf\n.\ncreateOrReplaceTempView\n(\n\"temp_data\"\n)\nspark\n.\nsql\n(\n\"\"\"\nINSERT INTO backend_reconciliation.daily_sales\nSELECT * FROM temp_data\n\"\"\"\n)\nOption 2: Using DataFrame API:\ndf\n=\nspark\n.\ncreateDataFrame\n(\n[\n(\n\"2024-06-01\"\n,\n\"electronics\"\n,\n12345.67\n)\n]\n,\n[\n\"sale_date\"\n,\n\"product_category\"\n,\n\"sales_amount\"\n]\n)\ndf\n.\nwriteTo\n(\n\"backend_reconciliation.daily_sales\"\n)\n\\\n.\nappend\n(\n)\nThe key differences from your original code are:\nSetting up the catalog configuration correctly with the account ID and proper catalog naming\nUsing the correct table reference format (namespace.table_name) without including the bucket name in the table path\nEnsuring your Glue job has the necessary IAM permissions to access the S3 table bucket\nMake sure to replace \"YOUR_ACCOUNT_ID\" with your actual AWS account ID in the configuration.\nSources\nIntroducing AWS Glue 5.0 for Apache Spark | AWS Big Data Blog\nNew Amazon S3 Tables: Storage optimized for analytics workloads | AWS News Blog\nRunning ETL jobs on Amazon S3 tables with AWS Glue - Amazon Simple Storage Service\n\nYou're right, finally I find it is the reason for the permissions, I should grant permissions in the Lake Formation console\n\nThanks for your response, while I change my code to this:\n`\nimport sys\nimport logging\nfrom pyspark.sql import SparkSession\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import lit\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nspark = SparkSession.builder.appName(\"WriteToS3Tables\")\n.config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n.config(\"spark.sql.defaultCatalog\", \"s3tables\")\n.config(\"spark.sql.catalog.s3tables\", \"org.apache.iceberg.spark.SparkCatalog\")\n.config(\"spark.sql.catalog.s3tables.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n.config(\"spark.sql.catalog.s3tables.client.region\", \"us-east-1\")\n.config(\"spark.sql.catalog.s3tables.glue.account-id\", \"abc\")\n.config(\"spark.sql.catalog.s3tables.glue.id\", \"abc:s3tablescatalog/test-s3-table-bucket\")\n.config(\"spark.sql.catalog.s3tables.warehouse\", \"s3://test-s3-table-bucket/\")\n.getOrCreate()\nglueContext = GlueContext(spark.sparkContext)\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\ndf = spark.createDataFrame([(\"2024-06-01\", \"electronics\", 12345.67)], [\"sale_date\", \"product_category\", \"sales_amount\"])\ndf.createOrReplaceTempView(\"temp_data\")\nspark.sql(\"\"\"\nINSERT INTO backend_reconciliation.daily_sales\nSELECT * FROM temp_data\n\"\"\")\nlogger.info(\"write complete.\")\njob.commit()\n`\nor this:\n`\nimport sys\nimport logging\nfrom pyspark.sql import SparkSession\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import lit\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nspark = SparkSession.builder.appName(\"WriteToS3Tables\")\n.config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n.config(\"spark.sql.defaultCatalog\", \"s3tables\")\n.config(\"spark.sql.catalog.s3tables\", \"org.apache.iceberg.spark.SparkCatalog\")\n.config(\"spark.sql.catalog.s3tables.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n.config(\"spark.sql.catalog.s3tables.client.region\", \"us-east-1\")\n.config(\"spark.sql.catalog.s3tables.glue.account-id\", \"abc\")\n.config(\"spark.sql.catalog.s3tables.glue.id\", \"abc:s3tablescatalog/test-s3-table-bucket\")\n.config(\"spark.sql.catalog.s3tables.warehouse\", \"s3://test-s3-table-bucket/\")\n.getOrCreate()\nglueContext = GlueContext(spark.sparkContext)\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\ndf = spark.createDataFrame([(\"2024-06-01\", \"electronics\", 12345.67)], [\"sale_date\", \"product_category\", \"sales_amount\"])\ndf.writeTo(\"backend_reconciliation.daily_sales\")\n.append()\nlogger.info(\"write complete.\")\njob.commit()\n`\nI still get this error:\nError Category: QUERY_ERROR; Failed Line Number: 34; Spark Error Class: TABLE_OR_VIEW_NOT_FOUND; AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view\nbackend_reconciliation\n.\ndaily_sales\ncannot be found. Verify the spelling and correctness of the schema and catalog.\n\"abc\" is my fake account id, and the Glue Job IAM role has: AmazonS3FullAccess, AmazonS3TablesFullAccess, AWSGlueServiceRole and CloudWatchLogsFullAccess permissions\nbtw, I want to point out that I'm using S3 table buckets, not general purpose bucket\n\nI added iceberg to datalake-format configuration, and it worked. Job details -> Advanced Properties -> Job parameters -> --datalake-formats: iceberg"
}